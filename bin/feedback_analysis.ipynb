{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "# import gensim\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import random\n",
    "# from gensim import utils\n",
    "# from gensim.models.doc2vec import LabeledSentence\n",
    "# from gensim.models import Doc2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import subprocess\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Han/Dropbox/CS565/Assignment_5_NLTK/data\n"
     ]
    }
   ],
   "source": [
    "cd ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1cbbbbe0f8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"output.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentiment_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'curl -d \"text='\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentiment_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\" http://text-processing.com/api/sentiment/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_file' is not defined"
     ]
    }
   ],
   "source": [
    "output_file = \"output.txt\"\n",
    "\n",
    "of = open(output_file, \"w\")\n",
    "sentiment_a = 'curl -d \"text='\n",
    "sentiment_b = '\" http://text-processing.com/api/sentiment/'\n",
    "\n",
    "for filename in [\"cond1.txt\",\"cond2.txt\",\"cond3.txt\",\"cond4.txt\"]:\n",
    "# def analysis(input_file):\n",
    "    \n",
    "#     filename = \"cond1.txt\"\n",
    "    cf = open(filename)\n",
    "    clines = cf.readlines()\n",
    "\n",
    "\n",
    "    print filename\n",
    "    of.write(filename+\"\\n\")\n",
    "\n",
    "\n",
    "    total_len = 0\n",
    "    eachline_len = []\n",
    "    Sentiment_Analysis = []\n",
    "    for each_line in clines:\n",
    "        each_line = each_line.replace('\"',' ')\n",
    "        curr_len = len(each_line)\n",
    "        eachline_len.append(curr_len)\n",
    "        total_len += curr_len\n",
    "        # how to access curl in python\n",
    "        tpm_sentiment = subprocess.check_output(sentiment_a+each_line+sentiment_b, shell=True)\n",
    "        Sentiment_Analysis.append(tpm_sentiment)\n",
    "#         print tpm_sentiment\n",
    "        \n",
    "\n",
    "    print \"Number of feedbacks: \"+str(len(clines))\n",
    "    of.write(\"Number of feedbacks: \"+str(len(clines))+\"\\n\")\n",
    "\n",
    "    \n",
    "    if len(eachline_len) != 0:\n",
    "        avg_len = reduce(lambda x, y: x + y, eachline_len) / len(eachline_len) \n",
    "    \n",
    "    print \"Avg length in condtion: \" + str(avg_len)\n",
    "    of.write(\"Avg length in condtion: \" + str(avg_len)+\"\\n\")\n",
    "\n",
    "    print \"Total Length of this condition: \" + str(total_len)\n",
    "    of.write(\"Total Length of this condition: \" + str(total_len)+\"\\n\")   \n",
    "    \n",
    "    print \"Length of each feedbacks: \" + str(eachline_len)\n",
    "    of.write(\"Length of each feedbacks: \" + str(eachline_len)+\"\\n\")\n",
    "\n",
    "    print \"Sentiment Analysis: \" + str(Sentiment_Analysis)\n",
    "    of.write(\"Sentiment Analysis: \" + str(Sentiment_Analysis)+\"\\n\")\n",
    "    \n",
    "\n",
    "    # clean the Stop words\n",
    "    text_new_list = []\n",
    "    for i in np.arange(len(clines)): \n",
    "        text_new_list.append(filter(lambda x : x.lower() not in stop_words, clines[i].split()))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print \"\\n\"\n",
    "    of.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "subprocess.check_output('curl -d \"text=The webkes see\" http://text-processing.com/api/sentiment/', shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
